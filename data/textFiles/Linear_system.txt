In systems theory , a linear system is a mathematical model of a system based on the use of a linear operator .
Linear systems typically exhibit features and properties that are much simpler than the nonlinear case.
As a mathematical abstraction or idealization, linear systems find important applications in automatic control theory, signal processing , and telecommunications . For example, the propagation medium for wireless communication systems can often be
modeled by linear systems.

A general deterministic system can be described by an operator, H , that maps an input, x ( t ) , as a function of t to an output, y ( t ) , a type of black box description.

A system is linear if and only if it satisfies the superposition principle , or equivalently both the additivity and homogeneity properties, without restrictions (that is, for all inputs, all scaling constants and all time.)

The superposition principle means that a linear combination of inputs to the system produces a linear combination of the individual zero-state outputs (that is, outputs setting the initial conditions to zero) corresponding to the individual inputs.

In a system that satisfies the homogeneity property, scaling the input always results in scaling the zero-state response by the same factor. In a system that satisfies the additivity property, adding two inputs always results in adding the corresponding two zero-state responses due to the individual inputs.

Mathematically, for a continuous-time system, given two arbitrary inputs x 1 ( t ) x 2 ( t ) {\displaystyle {\begin{aligned}x_{1}(t)\\x_{2}(t)\end{aligned}}} as well as their respective zero-state outputs y 1 ( t ) = H { x 1 ( t ) } y 2 ( t ) = H { x 2 ( t ) } {\displaystyle {\begin{aligned}y_{1}(t)&=H\left\{x_{1}(t)\right\}\\y_{2}(t)&=H\left\{x_{2}(t)\right\}\end{aligned}}} then a linear system must satisfy α y 1 ( t ) + β y 2 ( t ) = H { α x 1 ( t ) + β x 2 ( t ) } {\displaystyle \alpha y_{1}(t)+\beta y_{2}(t)=H\left\{\alpha x_{1}(t)+\beta x_{2}(t)\right\}} for any scalar values α and β , for any input signals x 1 ( t ) and x 2 ( t ) , and for all time t .

The system is then defined by the equation H ( x ( t )) = y ( t ) , where y ( t ) is some arbitrary function of time, and x ( t ) is the system state.  Given y ( t ) and H , the system can be solved for x ( t ) .

The behavior of the resulting system subjected to a complex input can be described as a sum of responses to simpler inputs.  In nonlinear systems, there is no such relation.  
This mathematical property makes the solution of modelling equations simpler than many nonlinear systems.
For time-invariant systems this is the basis of the impulse response or the frequency response methods (see LTI system theory ), which describe a general input function x ( t ) in terms of unit impulses or frequency components .

Typical differential equations of linear time-invariant systems are well adapted to analysis using the Laplace transform in the continuous case, and the Z-transform in the discrete case (especially in computer implementations).

Another perspective is that solutions to linear systems comprise a system of functions which act like vectors in the geometric sense.

A common use of linear models is to describe a nonlinear system by linearization .  This is usually done for mathematical convenience.

The previous definition of a linear system is applicable to SISO (single-input single-output) systems. For MIMO (multiple-input multiple-output) systems, input and output signal vectors ( x 1 ( t ) {\displaystyle {\mathbf {x} }_{1}(t)} , x 2 ( t ) {\displaystyle {\mathbf {x} }_{2}(t)} , y 1 ( t ) {\displaystyle {\mathbf {y} }_{1}(t)} , y 2 ( t ) {\displaystyle {\mathbf {y} }_{2}(t)} ) are considered instead of input and output signals ( x 1 ( t ) {\displaystyle x_{1}(t)} , x 2 ( t ) {\displaystyle x_{2}(t)} , y 1 ( t ) {\displaystyle y_{1}(t)} , y 2 ( t ) {\displaystyle y_{2}(t)} .)

This definition of a linear system is analogous to the definition of a linear differential equation in calculus , and a linear transformation in linear algebra .

A simple harmonic oscillator obeys the differential equation: m d 2 ( x ) d t 2 = − k x . {\displaystyle m{\frac {d^{2}(x)}{dt^{2}}}=-kx.}

If H ( x ( t ) ) = m d 2 ( x ( t ) ) d t 2 + k x ( t ) , {\displaystyle H(x(t))=m{\frac {d^{2}(x(t))}{dt^{2}}}+kx(t),} then H is a linear operator.  Letting y ( t ) = 0 , we can rewrite the differential equation as H ( x ( t )) = y ( t ) , which shows that a simple harmonic oscillator is a linear system.

Other examples of linear systems include those described by y ( t ) = k x ( t ) {\displaystyle y(t)=k\,x(t)} , y ( t ) = k d x ( t ) d t {\displaystyle y(t)=k\,{\frac {\mathrm {d} x(t)}{\mathrm {d} t}}} , y ( t ) = k ∫ − ∞ t x ( τ ) d τ {\displaystyle y(t)=k\,\int _{-\infty }^{t}x(\tau )\mathrm {d} \tau } , and any system described by ordinary linear differential equations. Systems described by y ( t ) = k {\displaystyle y(t)=k} , y ( t ) = k x ( t ) + k 0 {\displaystyle y(t)=k\,x(t)+k_{0}} , y ( t ) = sin ⁡ [ x ( t ) ] {\displaystyle y(t)=\sin {[x(t)]}} , y ( t ) = cos ⁡ [ x ( t ) ] {\displaystyle y(t)=\cos {[x(t)]}} , y ( t ) = x 2 ( t ) {\displaystyle y(t)=x^{2}(t)} , y ( t ) = x ( t ) {\textstyle y(t)={\sqrt {x(t)}}} , y ( t ) = | x ( t ) | {\displaystyle y(t)=|x(t)|} , and a system with odd-symmetry output consisting of a linear region and a saturation (constant) region, are non-linear because they don't always satisfy the superposition principle.

The output versus input graph of a linear system need not be a straight line through the origin. For example, consider a system described by y ( t ) = k d x ( t ) d t {\displaystyle y(t)=k\,{\frac {\mathrm {d} x(t)}{\mathrm {d} t}}} (such as a constant-capacitance capacitor or a constant-inductance inductor ). It is linear because it satisfies the superposition principle. However, when the input is a sinusoid, the output is also a sinusoid, and so its output-input plot is an ellipse centered at the origin rather than a straight line passing through the origin.

Also, the output of a linear system can contain harmonics (and have a smaller fundamental frequency than the input) even when the input is a sinusoid. For example, consider a system described by y ( t ) = ( 1.5 + cos ⁡ ( t ) ) x ( t ) {\displaystyle y(t)=(1.5+\cos {(t)})\,x(t)} . It is linear because it satisfies the superposition principle. However, when the input is a sinusoid of the form x ( t ) = cos ⁡ ( 3 t ) {\displaystyle x(t)=\cos {(3t)}} , using product-to-sum trigonometric identities it can be easily shown that the output is y ( t ) = 1.5 cos ⁡ ( 3 t ) + 0.5 cos ⁡ ( 2 t ) + 0.5 cos ⁡ ( 4 t ) {\displaystyle y(t)=1.5\cos {(3t)}+0.5\cos {(2t)}+0.5\cos {(4t)}} , that is, the output doesn't consist only of sinusoids of same frequency as the input ( 3 rad/s ), but instead also of sinusoids of frequencies 2 rad/s and 4 rad/s ; furthermore, taking the least common multiple of the fundamental period of the sinusoids of the output, it can be shown the fundamental angular frequency of the output is 1 rad/s , which is different than that of the input.

The time-varying impulse response h ( t 2 , t 1 ) of a linear system is defined as the response of the system at time t = t 2 to a single impulse applied at time t = t 1 . In other words, if the input x ( t ) to a linear system is x ( t ) = δ ( t − t 1 ) {\displaystyle x(t)=\delta (t-t_{1})} where δ( t ) represents the Dirac delta function , and the corresponding response y ( t ) of the system is y ( t = t 2 ) = h ( t 2 , t 1 ) {\displaystyle y(t=t_{2})=h(t_{2},t_{1})} then the function h ( t 2 , t 1 ) is the time-varying impulse response of the system. Since the system cannot respond before the input is applied the following causality condition must be satisfied: h ( t 2 , t 1 ) = 0 , t 2 < t 1 {\displaystyle h(t_{2},t_{1})=0,t_{2}<t_{1}}

The output of any general continuous-time linear system is related to the input by an integral which may be written over a doubly infinite range because of the causality condition: y ( t ) = ∫ − ∞ t h ( t , t ′ ) x ( t ′ ) d t ′ = ∫ − ∞ ∞ h ( t , t ′ ) x ( t ′ ) d t ′ {\displaystyle y(t)=\int _{-\infty }^{t}h(t,t')x(t')dt'=\int _{-\infty }^{\infty }h(t,t')x(t')dt'}

If the properties of the system do not depend on the time at which it is operated then it is said to be time-invariant and h is a function only of the time difference τ = t − t' which is zero for τ < 0 (namely t < t' ). By redefinition of h it is then possible to write the input-output relation equivalently in any of the ways, y ( t ) = ∫ − ∞ t h ( t − t ′ ) x ( t ′ ) d t ′ = ∫ − ∞ ∞ h ( t − t ′ ) x ( t ′ ) d t ′ = ∫ − ∞ ∞ h ( τ ) x ( t − τ ) d τ = ∫ 0 ∞ h ( τ ) x ( t − τ ) d τ {\displaystyle y(t)=\int _{-\infty }^{t}h(t-t')x(t')dt'=\int _{-\infty }^{\infty }h(t-t')x(t')dt'=\int _{-\infty }^{\infty }h(\tau )x(t-\tau )d\tau =\int _{0}^{\infty }h(\tau )x(t-\tau )d\tau }

Linear time-invariant systems are most commonly characterized by the Laplace transform of the impulse response function called  the transfer function which is: H ( s ) = ∫ 0 ∞ h ( t ) e − s t d t . {\displaystyle H(s)=\int _{0}^{\infty }h(t)e^{-st}\,dt.}

In applications this is usually a rational algebraic function of s . Because h ( t ) is zero for negative t , the integral may  equally be written over the doubly infinite range and putting s = iω follows the formula for the frequency response function : H ( i ω ) = ∫ − ∞ ∞ h ( t ) e − i ω t d t {\displaystyle H(i\omega )=\int _{-\infty }^{\infty }h(t)e^{-i\omega t}dt}

The output of any discrete time linear system is related to the input by the time-varying convolution sum: y [ n ] = ∑ m = − ∞ n h [ n , m ] x [ m ] = ∑ m = − ∞ ∞ h [ n , m ] x [ m ] {\displaystyle y[n]=\sum _{m=-\infty }^{n}{h[n,m]x[m]}=\sum _{m=-\infty }^{\infty }{h[n,m]x[m]}} or equivalently for a time-invariant system on redefining h , y [ n ] = ∑ k = 0 ∞ h [ k ] x [ n − k ] = ∑ k = − ∞ ∞ h [ k ] x [ n − k ] {\displaystyle y[n]=\sum _{k=0}^{\infty }{h[k]x[n-k]}=\sum _{k=-\infty }^{\infty }{h[k]x[n-k]}} where k = n − m {\displaystyle k=n-m} represents the lag time between the stimulus at time m and the response at time n .