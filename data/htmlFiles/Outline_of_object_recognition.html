<div id="mw-content-text" class="mw-body-content"><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Topical guide to object recognition</div>
<style data-mw-deduplicate="TemplateStyles:r1236090951">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}</style><div role="note" class="hatnote navigation-not-searchable">This article is about object recognition in computer vision. For object recognition in neuroscience, see <a href="/wiki/Cognitive_neuroscience_of_visual_object_recognition" class="mw-redirect" title="Cognitive neuroscience of visual object recognition">cognitive neuroscience of visual object recognition</a>.</div>
<p><small></small>
</p><p><b>Object recognition</b> – technology in the field of <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> for finding and identifying objects in an image or video sequence. Humans recognize a multitude of objects in images with little effort, despite the fact that the image of the objects may vary somewhat in different view points, in many different sizes and scales or even when they are translated or rotated. Objects can even be recognized when they are partially obstructed from view. This task is still a challenge for computer vision systems. Many approaches to the task have been implemented over multiple decades.
</p>
<style data-mw-deduplicate="TemplateStyles:r886046785">.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}</style><div class="toclimit-2"><meta property="mw:PageProp/toc"></div>
<div class="mw-heading mw-heading2"><h2 id="Approaches_based_on_CAD-like_object_models">Approaches based on CAD-like object models</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=1" title="Edit section: Approaches based on CAD-like object models"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><a href="/wiki/Edge_detection" title="Edge detection">Edge detection</a></li>
<li><a href="/wiki/Primal_sketch" class="mw-redirect" title="Primal sketch">Primal sketch</a></li>
<li>Marr, Mohan and Nevatia<sup id="cite_ref-1" class="reference"><a href="#cite_note-1"><span class="cite-bracket">[</span>1<span class="cite-bracket">]</span></a></sup></li>
<li>Lowe</li>
<li><a href="/wiki/Olivier_Faugeras" title="Olivier Faugeras">Olivier Faugeras</a></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Recognition_by_parts">Recognition by parts</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=2" title="Edit section: Recognition by parts"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><a href="/w/index.php?title=Generalized_cylinders&amp;action=edit&amp;redlink=1" class="new" title="Generalized cylinders (page does not exist)">Generalized cylinders</a> (<a href="/wiki/Thomas_Binford" title="Thomas Binford">Thomas Binford</a>)</li>
<li><a href="/wiki/Geon_(psychology)" title="Geon (psychology)">Geons</a> (<a href="/wiki/Irving_Biederman" title="Irving Biederman">Irving Biederman</a>)</li>
<li>Dickinson, Forsyth and Ponce</li></ul>
<div class="mw-heading mw-heading2"><h2 id="Appearance-based_methods">Appearance-based methods</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=3" title="Edit section: Appearance-based methods"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Use example images (called templates or exemplars) of the objects to perform recognition</li>
<li>Objects look different under varying conditions:
<ul><li>Changes in lighting or color</li>
<li>Changes in viewing direction</li>
<li>Changes in size/shape</li></ul></li>
<li>A single exemplar is unlikely to succeed reliably. However, it is impossible to represent all appearances of an object.</li></ul>
<div class="mw-heading mw-heading3"><h3 id="Edge_matching">Edge matching</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=4" title="Edit section: Edge matching"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Uses edge detection techniques, such as the <a href="/wiki/Canny_edge_detector" title="Canny edge detector">Canny edge detection</a>, to find edges.</li>
<li>Changes in lighting and color usually don't have much effect on image edges</li>
<li>Strategy:
<ol><li>Detect edges in template and image</li>
<li>Compare edges images to find the template</li>
<li>Must consider range of possible template positions</li></ol></li>
<li>Measurements:
<ul><li>Good –  count the number of overlapping edges. Not robust to changes in shape</li>
<li>Better – count the number of template edge pixels with some distance of an edge in the search image</li>
<li>Best –  determine probability distribution of distance to nearest edge in search image (if template at correct position). Estimate likelihood of each template position generating image</li></ul></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Divide-and-Conquer_search">Divide-and-Conquer search</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=5" title="Edit section: Divide-and-Conquer search"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Strategy:
<ul><li>Consider all positions as a set (a cell in the space of positions)</li>
<li>Determine lower bound on score at best position in cell</li>
<li>If bound is too large, prune cell</li>
<li>If bound is not too large, divide cell into subcells and try each subcell recursively</li>
<li>Process stops when cell is “small enough”</li></ul></li>
<li>Unlike multi-resolution search, this technique is guaranteed to find all matches that meet the criterion (assuming that the lower bound is accurate)</li>
<li>Finding the Bound:
<ul><li>To find the lower bound on the best score, look at score for the template position represented by the center of the cell</li>
<li>Subtract maximum change from the “center” position for any other position in cell (occurs at cell corners)</li></ul></li>
<li>Complexities arise from determining bounds on distance<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2022)">citation needed</span></a></i>]</sup></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Greyscale_matching">Greyscale matching</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=6" title="Edit section: Greyscale matching"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Edges are (mostly) robust to illumination changes, however they throw away a lot of information</li>
<li>Must compute pixel distance as a function of both pixel position and pixel intensity</li>
<li>Can be applied to color also</li></ul>
<div class="mw-heading mw-heading3"><h3 id="Gradient_matching">Gradient matching</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=7" title="Edit section: Gradient matching"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Another way to be robust to illumination changes without throwing away as much information is to compare image gradients</li>
<li>Matching is performed like matching greyscale images</li>
<li>Simple alternative: Use (normalized) correlation</li></ul>
<div class="mw-heading mw-heading3"><h3 id="Histograms_of_receptive_field_responses">Histograms of receptive field responses</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=8" title="Edit section: Histograms of receptive field responses"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Avoids explicit point correspondences</li>
<li>Relations between different image points implicitly coded in the receptive field responses</li>
<li>Swain and Ballard (1991),<sup id="cite_ref-2" class="reference"><a href="#cite_note-2"><span class="cite-bracket">[</span>2<span class="cite-bracket">]</span></a></sup> Schiele and Crowley (2000),<sup id="cite_ref-3" class="reference"><a href="#cite_note-3"><span class="cite-bracket">[</span>3<span class="cite-bracket">]</span></a></sup> Linde and Lindeberg (2004, 2012)<sup id="cite_ref-4" class="reference"><a href="#cite_note-4"><span class="cite-bracket">[</span>4<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-5" class="reference"><a href="#cite_note-5"><span class="cite-bracket">[</span>5<span class="cite-bracket">]</span></a></sup></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Large_modelbases">Large modelbases</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=9" title="Edit section: Large modelbases"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>One approach to efficiently searching the database for a specific image to use eigenvectors of the templates (called <a href="/wiki/Eigenface" title="Eigenface">eigenfaces</a>)</li>
<li>Modelbases are a collection of geometric models of the objects that should be recognized</li></ul>
<div class="mw-heading mw-heading2"><h2 id="Feature-based_methods">Feature-based methods</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=10" title="Edit section: Feature-based methods"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1129693374">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1246091330">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}}</style><table class="sidebar nomobile nowraplinks"><tbody><tr><th class="sidebar-title"><a href="/wiki/Feature_(computer_vision)" title="Feature (computer vision)">Feature detection</a></th></tr><tr><th class="sidebar-heading">
<a href="/wiki/Edge_detection" title="Edge detection">Edge detection</a></th></tr><tr><td class="sidebar-content hlist">
<ul><li><a href="/wiki/Canny_edge_detector" title="Canny edge detector">Canny</a></li>
<li><a href="/wiki/Deriche_edge_detector" title="Deriche edge detector">Deriche</a></li>
<li><a href="/wiki/Edge_detection#Differential" title="Edge detection">Differential</a></li>
<li><a href="/wiki/Sobel_operator" title="Sobel operator">Sobel</a></li>
<li><a href="/wiki/Prewitt_operator" title="Prewitt operator">Prewitt</a></li>
<li><a href="/wiki/Robinson_compass_mask" title="Robinson compass mask">Robinson</a></li>
<li><a href="/wiki/Roberts_cross" title="Roberts cross">Roberts cross</a></li></ul></td>
</tr><tr><th class="sidebar-heading">
<a href="/wiki/Corner_detection" title="Corner detection">Corner detection</a></th></tr><tr><td class="sidebar-content hlist">
<ul><li><a href="/wiki/Corner_detection#The_Harris_&amp;_Stephens_/_Shi%E2%80%93Tomasi_corner_detection_algorithms" title="Corner detection">Harris operator</a></li>
<li><a href="/wiki/Corner_detection#The_Harris_&amp;_Stephens_/_Shi%E2%80%93Tomasi_corner_detection_algorithms" title="Corner detection">Shi and Tomasi</a></li>
<li><a href="/wiki/Corner_detection#The_level_curve_curvature_approach" title="Corner detection">Level curve curvature</a></li>
<li><a href="/wiki/Corner_detection#Scale-space_interest_points_based_on_the_Lindeberg_Hessian_feature_strength_measures" title="Corner detection">Hessian feature strength measures</a></li>
<li><a href="/wiki/Corner_detection#The_SUSAN_corner_detector" title="Corner detection">SUSAN</a></li>
<li><a href="/wiki/Corner_detection#AST-based_feature_detectors" title="Corner detection">FAST</a></li></ul></td>
</tr><tr><th class="sidebar-heading">
<a href="/wiki/Blob_detection" title="Blob detection">Blob detection</a></th></tr><tr><td class="sidebar-content hlist">
<ul><li><a href="/wiki/Blob_detection#The_Laplacian_of_Gaussian" title="Blob detection">Laplacian of Gaussian (LoG)</a></li>
<li><a href="/wiki/Difference_of_Gaussians" title="Difference of Gaussians">Difference of Gaussians (DoG)</a></li>
<li><a href="/wiki/Blob_detection#The_determinant_of_the_Hessian" title="Blob detection">Determinant of Hessian  (DoH)</a></li>
<li><a href="/wiki/Maximally_stable_extremal_regions" title="Maximally stable extremal regions">Maximally stable extremal regions</a></li>
<li><a href="/wiki/Principal_Curvature-Based_Region_Detector" class="mw-redirect" title="Principal Curvature-Based Region Detector">PCBR</a></li></ul></td>
</tr><tr><th class="sidebar-heading">
<a href="/wiki/Ridge_detection" title="Ridge detection">Ridge detection</a></th></tr><tr><th class="sidebar-heading">
Hough transform</th></tr><tr><td class="sidebar-content hlist">
<ul><li><a href="/wiki/Hough_transform" title="Hough transform">Hough transform</a></li>
<li><a href="/wiki/Generalized_Hough_transform" class="mw-redirect" title="Generalized Hough transform">Generalized Hough transform</a></li></ul></td>
</tr><tr><th class="sidebar-heading">
Structure tensor</th></tr><tr><td class="sidebar-content hlist">
<ul><li><a href="/wiki/Structure_tensor" title="Structure tensor">Structure tensor</a></li>
<li><a href="/wiki/Generalized_structure_tensor" title="Generalized structure tensor">Generalized structure tensor</a></li></ul></td>
</tr><tr><th class="sidebar-heading">
Affine invariant feature detection</th></tr><tr><td class="sidebar-content hlist">
<ul><li><a href="/wiki/Affine_shape_adaptation" title="Affine shape adaptation">Affine shape adaptation</a></li>
<li><a href="/wiki/Harris_affine_region_detector" title="Harris affine region detector">Harris affine</a></li>
<li><a href="/wiki/Hessian_affine_region_detector" title="Hessian affine region detector">Hessian affine</a></li></ul></td>
</tr><tr><th class="sidebar-heading">
Feature description</th></tr><tr><td class="sidebar-content hlist">
<ul><li><a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a></li>
<li><a href="/wiki/Speeded_up_robust_features" title="Speeded up robust features">SURF</a></li>
<li><a href="/wiki/GLOH" title="GLOH">GLOH</a></li>
<li><a href="/wiki/Histogram_of_oriented_gradients" title="Histogram of oriented gradients">HOG</a></li></ul></td>
</tr><tr><th class="sidebar-heading">
<a href="/wiki/Scale_space" title="Scale space">Scale space</a></th></tr><tr><td class="sidebar-content hlist">
<ul><li><a href="/wiki/Scale-space_axioms" title="Scale-space axioms">Scale-space axioms</a></li>
<li><a href="/wiki/Scale_space_implementation" title="Scale space implementation">Implementation details</a></li>
<li><a href="/wiki/Pyramid_(image_processing)" title="Pyramid (image processing)">Pyramids</a></li></ul></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1239400231">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Feature_detection_(computer_vision)_navbox" title="Template:Feature detection (computer vision) navbox"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Feature_detection_(computer_vision)_navbox" title="Template talk:Feature detection (computer vision) navbox"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Feature_detection_(computer_vision)_navbox" title="Special:EditPage/Template:Feature detection (computer vision) navbox"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<ul><li>a search is used to find feasible matches between object features and <a href="/wiki/Image_feature" class="mw-redirect" title="Image feature">image features</a>.</li>
<li>the primary constraint is that a single position of the object must account for all of the feasible matches.</li>
<li>methods that <a href="/wiki/Feature_extraction" class="mw-redirect" title="Feature extraction">extract features</a> from the objects to be recognized and the images to be searched.
<ul><li>surface patches</li>
<li>corners</li>
<li>linear edges</li></ul></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Interpretation_trees">Interpretation trees</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=11" title="Edit section: Interpretation trees"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>A method for searching for feasible matches, is to search through a tree.</li>
<li>Each node in the tree represents a set of matches.
<ul><li>Root node represents empty set</li>
<li>Each other node is the union of the matches in the parent node and one additional match.</li>
<li>Wildcard is used for features with no match</li></ul></li>
<li>Nodes are “pruned” when the set of matches is infeasible.
<ul><li>A pruned node has no children</li></ul></li>
<li>Historically significant and still used, but less commonly</li></ul>
<div class="mw-heading mw-heading3"><h3 id="Hypothesize_and_test">Hypothesize and test</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=12" title="Edit section: Hypothesize and test"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>General Idea:
<ul><li>Hypothesize a <a href="/wiki/Correspondence_problem" title="Correspondence problem">correspondence</a> between a collection of image features and a collection of object features</li>
<li>Then use this to generate a hypothesis about the projection from the object coordinate frame to the image frame</li>
<li>Use this projection hypothesis to generate a rendering of the object. This step is usually known as backprojection</li>
<li>Compare the rendering to the image, and, if the two are sufficiently similar, accept the hypothesis</li></ul></li>
<li>Obtaining Hypothesis:
<ul><li>There are a variety of different ways of generating hypotheses.</li>
<li>When camera intrinsic parameters are known, the hypothesis is equivalent to a hypothetical position and orientation – <a href="/wiki/Pose_(computer_vision)" title="Pose (computer vision)">pose</a> – for the object.</li>
<li>Utilize geometric constraints</li>
<li>Construct a correspondence for small sets of object features to every correctly sized subset of image points. (These are the hypotheses)</li></ul></li>
<li>Three basic approaches:
<ul><li>Obtaining Hypotheses by Pose Consistency</li>
<li>Obtaining Hypotheses by Pose Clustering</li>
<li>Obtaining Hypotheses by Using Invariants</li></ul></li>
<li>Expense search that is also redundant, but can be improved using Randomization and/or Grouping
<ul><li>Randomization
<ul><li>Examining small sets of image features until likelihood of missing object becomes small</li>
<li>For each set of image features, all possible matching sets of model features must be considered.</li>
<li>Formula:
<dl><dd>(1 – W<sup>c</sup>)<sup>k</sup> = Z</dd></dl>
<ul><li>W =  the fraction of image points that are “good” (w ~ m/n)</li>
<li>c =  the number of correspondences necessary</li>
<li>k =  the number of trials</li>
<li>Z =  the probability of every trial using one (or more) incorrect correspondences</li></ul></li></ul></li>
<li>Grouping
<ul><li>If we can determine groups of points that are likely to come from the same object, we can reduce the number of hypotheses that need to be examined</li></ul></li></ul></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Pose_consistency">Pose consistency</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=13" title="Edit section: Pose consistency"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Also called Alignment, since the object is being aligned to the image</li>
<li>Correspondences between image features and model features are not independent – Geometric constraints</li>
<li>A small number of correspondences yields the object position – the others must be consistent with this</li>
<li>General Idea:
<ul><li>If we hypothesize a match between a sufficiently large group of image features and a sufficiently large group of object features, then we can recover the missing camera parameters from this hypothesis (and so render the rest of the object)</li></ul></li>
<li>Strategy:
<ul><li>Generate hypotheses using small number of correspondences (e.g. triples of points for 3D recognition)</li>
<li>Project other model features into image (<a href="/wiki/Backprojection" class="mw-redirect" title="Backprojection">backproject</a>) and verify additional correspondences</li></ul></li>
<li>Use the smallest number of correspondences necessary to achieve discrete object poses</li></ul>
<div class="mw-heading mw-heading3"><h3 id="Pose_clustering"><a href="/wiki/Pose_clustering" class="mw-redirect" title="Pose clustering">Pose clustering</a></h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=14" title="Edit section: Pose clustering"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>General Idea:
<ul><li>Each object leads to many correct sets of correspondences, each of which has (roughly) the same pose</li>
<li>Vote on pose. Use an accumulator array that represents pose space for each object</li>
<li>This is essentially a <a href="/wiki/Hough_transform" title="Hough transform">Hough transform</a></li></ul></li>
<li>Strategy:
<ul><li>For each object, set up an accumulator array that represents pose space – each element in the accumulator array corresponds to a “bucket” in pose space.</li>
<li>Then take each image frame group, and hypothesize a correspondence between it and every frame group on every object</li>
<li>For each of these correspondences, determine pose parameters and make an entry in the accumulator array for the current object at the pose value.</li>
<li>If there are large numbers of votes in any object's accumulator array, this can be interpreted as evidence for the presence of that object at that pose.</li>
<li>The evidence can be checked using a verification method</li></ul></li>
<li>Note that this method uses sets of correspondences, rather than individual correspondences
<ul><li>Implementation is easier, since each set yields a small number of possible object poses.</li></ul></li>
<li>Improvement
<ul><li>The noise resistance of this method can be improved by not counting votes for objects at poses where the vote is obviously unreliable</li></ul>
<dl><dd>§ For example, in cases where, if the object was at that pose, the object frame group would be invisible.</dd></dl>
<ul><li>These improvements are sufficient to yield working systems</li></ul></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Invariance"><a href="/wiki/Invariant_(physics)" title="Invariant (physics)">Invariance</a></h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=15" title="Edit section: Invariance"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>There are geometric properties that are invariant to camera transformations</li>
<li>Most easily developed for images of planar objects, but can be applied to other cases as well</li></ul>
<div class="mw-heading mw-heading3"><h3 id="Geometric_hashing"><a href="/wiki/Geometric_hashing" title="Geometric hashing">Geometric hashing</a></h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=16" title="Edit section: Geometric hashing"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>An algorithm that uses geometric invariants to vote for object hypotheses</li>
<li>Similar to pose clustering, however instead of voting on pose, we are now voting on geometry</li>
<li>A technique originally developed for matching geometric features (uncalibrated affine views of plane models) against a database of such features</li>
<li>Widely used for pattern-matching, CAD/CAM, and medical imaging.</li>
<li>It is difficult to choose the size of the buckets</li>
<li>It is hard to be sure what “enough” means. Therefore, there may be some danger that the table will get clogged.</li></ul>
<div class="mw-heading mw-heading3"><h3 id="Scale-invariant_feature_transform_(SIFT)"><span id="Scale-invariant_feature_transform_.28SIFT.29"></span><a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">Scale-invariant feature transform</a> (SIFT)</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=17" title="Edit section: Scale-invariant feature transform (SIFT)"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Keypoints of objects are first extracted from a set of reference images and stored in a database</li>
<li>An object is recognized in a new image by individually comparing each feature from the new image to this database and finding candidate matching features based on Euclidean distance of their feature vectors.</li>
<li>Lowe (2004)<sup id="cite_ref-6" class="reference"><a href="#cite_note-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-Lindeberg2012_7-0" class="reference"><a href="#cite_note-Lindeberg2012-7"><span class="cite-bracket">[</span>7<span class="cite-bracket">]</span></a></sup></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Speeded_Up_Robust_Features_(SURF)"><span id="Speeded_Up_Robust_Features_.28SURF.29"></span><a href="/wiki/Speeded_Up_Robust_Features" class="mw-redirect" title="Speeded Up Robust Features">Speeded Up Robust Features</a> (SURF)</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=18" title="Edit section: Speeded Up Robust Features (SURF)"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>A robust image detector &amp; descriptor</li>
<li>The standard version is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT</li>
<li>Based on sums of approximated <a href="/wiki/Haar-like_feature" title="Haar-like feature">2D Haar wavelet responses</a> and made efficient use of integral images.</li>
<li>Bay et al. (2008)<sup id="cite_ref-8" class="reference"><a href="#cite_note-8"><span class="cite-bracket">[</span>8<span class="cite-bracket">]</span></a></sup></li></ul>
<div class="mw-heading mw-heading3"><h3 id="Bag_of_words_representations">Bag of words representations</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=19" title="Edit section: Bag of words representations"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1236090951"><div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Bag_of_words_model_in_computer_vision" class="mw-redirect" title="Bag of words model in computer vision">Bag of words model in computer vision</a></div>
<div class="mw-heading mw-heading2"><h2 id="Genetic_algorithm">Genetic algorithm</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=20" title="Edit section: Genetic algorithm"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p><a href="/wiki/Genetic_algorithm" title="Genetic algorithm">Genetic algorithms</a> can operate without prior knowledge of a given dataset and can develop recognition procedures without human intervention. A recent project achieved 100 percent accuracy on the benchmark motorbike, face, airplane and car image datasets from Caltech and 99.4 percent accuracy on fish species image datasets.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9"><span class="cite-bracket">[</span>9<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-10" class="reference"><a href="#cite_note-10"><span class="cite-bracket">[</span>10<span class="cite-bracket">]</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Other_approaches">Other approaches</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=21" title="Edit section: Other approaches"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><a href="/wiki/3D_object_recognition" title="3D object recognition">3D object recognition</a> and <a href="/wiki/3D_reconstruction" title="3D reconstruction">reconstruction</a><sup id="cite_ref-11" class="reference"><a href="#cite_note-11"><span class="cite-bracket">[</span>11<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Biologically_inspired" class="mw-redirect" title="Biologically inspired">Biologically inspired</a> object recognition</li>
<li><a href="/wiki/Artificial_neural_networks" class="mw-redirect" title="Artificial neural networks">Artificial neural networks</a> and <a href="/wiki/Deep_Learning" class="mw-redirect" title="Deep Learning">Deep Learning</a> especially <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural networks</a></li>
<li><a href="/wiki/Context_awareness" title="Context awareness">Context</a><sup id="cite_ref-Aude_12-0" class="reference"><a href="#cite_note-Aude-12"><span class="cite-bracket">[</span>12<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-Niu_13-0" class="reference"><a href="#cite_note-Niu-13"><span class="cite-bracket">[</span>13<span class="cite-bracket">]</span></a></sup></li>
<li>Explicit and <a href="/wiki/Implicit_surface" title="Implicit surface">implicit</a> 3D object models</li>
<li><a href="/w/index.php?title=Fast_indexing&amp;action=edit&amp;redlink=1" class="new" title="Fast indexing (page does not exist)">Fast indexing</a><sup id="cite_ref-14" class="reference"><a href="#cite_note-14"><span class="cite-bracket">[</span>14<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/w/index.php?title=Global_scene_representations&amp;action=edit&amp;redlink=1" class="new" title="Global scene representations (page does not exist)">Global scene representations</a><sup id="cite_ref-Aude_12-1" class="reference"><a href="#cite_note-Aude-12"><span class="cite-bracket">[</span>12<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/w/index.php?title=Gradient_histograms&amp;action=edit&amp;redlink=1" class="new" title="Gradient histograms (page does not exist)">Gradient histograms</a></li>
<li><a href="/wiki/Stochastic_grammar" title="Stochastic grammar">Stochastic grammars</a><sup id="cite_ref-15" class="reference"><a href="#cite_note-15"><span class="cite-bracket">[</span>15<span class="cite-bracket">]</span></a></sup></li>
<li>Intraclass <a href="/wiki/Transfer_learning" title="Transfer learning">transfer learning</a></li>
<li><a href="/wiki/Object_categorization_from_image_search" title="Object categorization from image search">Object categorization from image search</a></li>
<li><a href="/wiki/Reflectance" title="Reflectance">Reflectance</a><sup id="cite_ref-16" class="reference"><a href="#cite_note-16"><span class="cite-bracket">[</span>16<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Shape_from_Shading" class="mw-redirect" title="Shape from Shading">Shape-from-shading</a><sup id="cite_ref-17" class="reference"><a href="#cite_note-17"><span class="cite-bracket">[</span>17<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Template_matching" title="Template matching">Template matching</a></li>
<li>Texture<sup id="cite_ref-Shotton_18-0" class="reference"><a href="#cite_note-Shotton-18"><span class="cite-bracket">[</span>18<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Topic_model" title="Topic model">Topic models</a><sup id="cite_ref-Niu_13-1" class="reference"><a href="#cite_note-Niu-13"><span class="cite-bracket">[</span>13<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/w/index.php?title=Window-based_detection&amp;action=edit&amp;redlink=1" class="new" title="Window-based detection (page does not exist)">Window-based detection</a></li>
<li><a href="/w/index.php?title=Deformable_Part_Model&amp;action=edit&amp;redlink=1" class="new" title="Deformable Part Model (page does not exist)">Deformable Part Model</a></li>
<li><a href="/wiki/Bingham_distribution" title="Bingham distribution">Bingham distribution</a><sup id="cite_ref-19" class="reference"><a href="#cite_note-19"><span class="cite-bracket">[</span>19<span class="cite-bracket">]</span></a></sup></li></ul>
<div class="mw-heading mw-heading2"><h2 id="Applications">Applications</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=22" title="Edit section: Applications"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Object recognition methods has the following applications:
</p>
<ul><li><a href="/wiki/Activity_recognition" title="Activity recognition">Activity recognition</a><sup id="cite_ref-20" class="reference"><a href="#cite_note-20"><span class="cite-bracket">[</span>20<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Automatic_image_annotation" title="Automatic image annotation">Automatic image annotation</a><sup id="cite_ref-21" class="reference"><a href="#cite_note-21"><span class="cite-bracket">[</span>21<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-22" class="reference"><a href="#cite_note-22"><span class="cite-bracket">[</span>22<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Automatic_target_recognition" title="Automatic target recognition">Automatic target recognition</a></li>
<li><a href="/w/index.php?title=Android_Eyes_-_Object_Recognition&amp;action=edit&amp;redlink=1" class="new" title="Android Eyes - Object Recognition (page does not exist)">Android Eyes - Object Recognition</a><sup id="cite_ref-23" class="reference"><a href="#cite_note-23"><span class="cite-bracket">[</span>23<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Computer-aided_diagnosis" title="Computer-aided diagnosis">Computer-aided diagnosis</a><sup id="cite_ref-24" class="reference"><a href="#cite_note-24"><span class="cite-bracket">[</span>24<span class="cite-bracket">]</span></a></sup></li>
<li>Image <a href="/wiki/Panorama" title="Panorama">panoramas</a><sup id="cite_ref-25" class="reference"><a href="#cite_note-25"><span class="cite-bracket">[</span>25<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Image_watermarking" class="mw-redirect" title="Image watermarking">Image watermarking</a><sup id="cite_ref-26" class="reference"><a href="#cite_note-26"><span class="cite-bracket">[</span>26<span class="cite-bracket">]</span></a></sup></li>
<li>Global <a href="/wiki/Robot_localization" class="mw-redirect" title="Robot localization">robot localization</a><sup id="cite_ref-27" class="reference"><a href="#cite_note-27"><span class="cite-bracket">[</span>27<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Face_detection" title="Face detection">Face detection</a><sup id="cite_ref-28" class="reference"><a href="#cite_note-28"><span class="cite-bracket">[</span>28<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Optical_Character_Recognition" class="mw-redirect" title="Optical Character Recognition">Optical Character Recognition</a><sup id="cite_ref-29" class="reference"><a href="#cite_note-29"><span class="cite-bracket">[</span>29<span class="cite-bracket">]</span></a></sup></li>
<li>Manufacturing <a href="/wiki/Quality_control" title="Quality control">quality control</a><sup id="cite_ref-30" class="reference"><a href="#cite_note-30"><span class="cite-bracket">[</span>30<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Content-based_image_retrieval" title="Content-based image retrieval">Content-based image retrieval</a><sup id="cite_ref-31" class="reference"><a href="#cite_note-31"><span class="cite-bracket">[</span>31<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/w/index.php?title=Object_Counting_and_Monitoring&amp;action=edit&amp;redlink=1" class="new" title="Object Counting and Monitoring (page does not exist)">Object Counting and Monitoring</a><sup id="cite_ref-32" class="reference"><a href="#cite_note-32"><span class="cite-bracket">[</span>32<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Automated_parking_system" title="Automated parking system">Automated parking systems</a><sup id="cite_ref-33" class="reference"><a href="#cite_note-33"><span class="cite-bracket">[</span>33<span class="cite-bracket">]</span></a></sup></li>
<li>Visual Positioning and <a href="/wiki/Video_tracking" title="Video tracking">tracking</a><sup id="cite_ref-34" class="reference"><a href="#cite_note-34"><span class="cite-bracket">[</span>34<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Video_stabilization" class="mw-redirect" title="Video stabilization">Video stabilization</a><sup id="cite_ref-35" class="reference"><a href="#cite_note-35"><span class="cite-bracket">[</span>35<span class="cite-bracket">]</span></a></sup></li>
<li><a href="/wiki/Pedestrian_detection" title="Pedestrian detection">Pedestrian detection</a></li>
<li><a href="/wiki/Intelligent_speed_assistance" title="Intelligent speed assistance">Intelligent speed assistance</a> (in car and other vehicles)</li></ul>
<div class="mw-heading mw-heading2"><h2 id="Surveys">Surveys</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=23" title="Edit section: Surveys"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Daniilides and Eklundh, Edelman.</li>
<li><style data-mw-deduplicate="TemplateStyles:r1238218222">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite id="CITEREFRoth,_Peter_M.Winter,_Martin2008" class="citation journal cs1 cs1-prop-long-vol">Roth, Peter M. &amp; Winter, Martin (2008). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20150921140625/http://lrs.icg.tugraz.at/pubs/roth_tr-icg_08.pdf">"SURVEYOFAPPEARANCE-BASED METHODS FOR OBJECT RECOGNITION"</a> <span class="cs1-format">(PDF)</span>. <i>Technical Report</i>. ICG-TR-01/08. Archived from <a rel="nofollow" class="external text" href="http://lrs.icg.tugraz.at/pubs/roth_tr-icg_08.pdf">the original</a> <span class="cs1-format">(PDF)</span> on 2015-09-21<span class="reference-accessdate">. Retrieved <span class="nowrap">2016-02-26</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Technical+Report&amp;rft.atitle=SURVEYOFAPPEARANCE-BASED+METHODS+FOR+OBJECT+RECOGNITION&amp;rft.volume=ICG-TR-01%2F08&amp;rft.date=2008&amp;rft.au=Roth%2C+Peter+M.&amp;rft.au=Winter%2C+Martin&amp;rft_id=http%3A%2F%2Flrs.icg.tugraz.at%2Fpubs%2Froth_tr-icg_08.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></li></ul>
<div class="mw-heading mw-heading2"><h2 id="See_also">See also</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=24" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><a href="/wiki/Histogram_of_oriented_gradients" title="Histogram of oriented gradients">Histogram of oriented gradients</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li>
<li><a href="/wiki/OpenCV" title="OpenCV">OpenCV</a></li>
<li><a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">Scale-invariant feature transform</a> (SIFT)</li>
<li><a href="/wiki/Object_detection" title="Object detection">Object detection</a></li>
<li><a rel="nofollow" class="external text" href="http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform">Scholarpedia article on scale-invariant feature transform and related object recognition methods</a></li>
<li><a href="/wiki/Speeded_up_robust_features" title="Speeded up robust features">SURF</a></li>
<li><a href="/wiki/Template_matching" title="Template matching">Template matching</a></li>
<li><a href="/wiki/Integral_channel_feature" title="Integral channel feature">Integral channel feature</a></li></ul>
<dl><dt>Lists</dt></dl>
<ul><li><a href="/wiki/List_of_computer_vision_topics" class="mw-redirect" title="List of computer vision topics">List of computer vision topics</a></li>
<li><a href="/wiki/List_of_emerging_technologies" title="List of emerging technologies">List of emerging technologies</a></li>
<li><a href="/wiki/Outline_of_artificial_intelligence" title="Outline of artificial intelligence">Outline of artificial intelligence</a></li></ul>
<div class="mw-heading mw-heading2"><h2 id="Notes">Notes</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=25" title="Edit section: Notes"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1239543626">.mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist reflist-columns references-column-width reflist-columns-2">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFRahesh_MohanRakamant_Nevatia1992" class="citation journal cs1">Rahesh Mohan &amp; Rakamant Nevatia (1992). <a rel="nofollow" class="external text" href="http://iris.usc.edu/outlines/papers/1992/pami-mohan-92.pdf">"Perceptual organization for scene segmentation and description"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Trans Pattern Anal Mach Intell</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Trans+Pattern+Anal+Mach+Intell&amp;rft.atitle=Perceptual+organization+for+scene+segmentation+and+description&amp;rft.date=1992&amp;rft.au=Rahesh+Mohan&amp;rft.au=Rakamant+Nevatia&amp;rft_id=http%3A%2F%2Firis.usc.edu%2Foutlines%2Fpapers%2F1992%2Fpami-mohan-92.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFSwainBallard1991" class="citation journal cs1">Swain, Michael J.; Ballard, Dana H. (1991-11-01). <a rel="nofollow" class="external text" href="https://doi.org/10.1007/BF00130487">"Color indexing"</a>. <i>International Journal of Computer Vision</i>. <b>7</b> (1): 11–32. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2FBF00130487">10.1007/BF00130487</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a> <a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1573-1405">1573-1405</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:8167136">8167136</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Computer+Vision&amp;rft.atitle=Color+indexing&amp;rft.volume=7&amp;rft.issue=1&amp;rft.pages=11-32&amp;rft.date=1991-11-01&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A8167136%23id-name%3DS2CID&amp;rft.issn=1573-1405&amp;rft_id=info%3Adoi%2F10.1007%2FBF00130487&amp;rft.aulast=Swain&amp;rft.aufirst=Michael+J.&amp;rft.au=Ballard%2C+Dana+H.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1007%2FBF00130487&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFSchieleCrowley2000" class="citation journal cs1">Schiele, Bernt; Crowley, James L. (2000-01-01). <a rel="nofollow" class="external text" href="https://doi.org/10.1023/A:1008120406972">"Recognition without Correspondence using Multidimensional Receptive Field Histograms"</a>. <i>International Journal of Computer Vision</i>. <b>36</b> (1): 31–50. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1023%2FA%3A1008120406972">10.1023/A:1008120406972</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a> <a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1573-1405">1573-1405</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:2551159">2551159</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Computer+Vision&amp;rft.atitle=Recognition+without+Correspondence+using+Multidimensional+Receptive+Field+Histograms&amp;rft.volume=36&amp;rft.issue=1&amp;rft.pages=31-50&amp;rft.date=2000-01-01&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A2551159%23id-name%3DS2CID&amp;rft.issn=1573-1405&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1008120406972&amp;rft.aulast=Schiele&amp;rft.aufirst=Bernt&amp;rft.au=Crowley%2C+James+L.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1023%2FA%3A1008120406972&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="ftp://ftp1.nada.kth.se/pub/documents/CVAP/reports/LinLin04-ICPR.pdf">O. Linde and T. Lindeberg "Object recognition using composed receptive field histograms of higher dimensionality", Proc. International Conference on Pattern Recognition (ICPR'04), Cambridge, U.K. II:1-6, 2004.</a></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFO._LindeT._Lindeberg2012" class="citation journal cs1">O. Linde; T. Lindeberg (2012). <a rel="nofollow" class="external text" href="http://www.csc.kth.se/~tony/abstracts/LinLin11-CompComplCueHist.html">"Composed Complex-Cue Histograms: An Investigation of the Information Content in Receptive Field Based Image Descriptors for Object Recognition"</a>. <i>Computer Vision and Image Understanding</i>. <b>116</b> (4): 538–560. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.cviu.2011.12.003">10.1016/j.cviu.2011.12.003</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computer+Vision+and+Image+Understanding&amp;rft.atitle=Composed+Complex-Cue+Histograms%3A+An+Investigation+of+the+Information+Content+in+Receptive+Field+Based+Image+Descriptors+for+Object+Recognition&amp;rft.volume=116&amp;rft.issue=4&amp;rft.pages=538-560&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cviu.2011.12.003&amp;rft.au=O.+Linde&amp;rft.au=T.+Lindeberg&amp;rft_id=http%3A%2F%2Fwww.csc.kth.se%2F~tony%2Fabstracts%2FLinLin11-CompComplCueHist.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://citeseer.ist.psu.edu/lowe04distinctive.html">Lowe, D. G., "Distinctive image features from scale-invariant keypoints", International Journal of Computer Vision, 60, 2, pp. 91-110, 2004.</a></span>
</li>
<li id="cite_note-Lindeberg2012-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-Lindeberg2012_7-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFLindeberg,_Tony2012" class="citation journal cs1">Lindeberg, Tony (2012). <a rel="nofollow" class="external text" href="https://doi.org/10.4249%2Fscholarpedia.10491">"Scale invariant feature transform"</a>. <i>Scholarpedia</i>. <b>7</b> (5): 10491. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2012SchpJ...710491L">2012SchpJ...710491L</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.4249%2Fscholarpedia.10491">10.4249/scholarpedia.10491</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Scholarpedia&amp;rft.atitle=Scale+invariant+feature+transform&amp;rft.volume=7&amp;rft.issue=5&amp;rft.pages=10491&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.4249%2Fscholarpedia.10491&amp;rft_id=info%3Abibcode%2F2012SchpJ...710491L&amp;rft.au=Lindeberg%2C+Tony&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.4249%252Fscholarpedia.10491&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFBayEssTuytelaarsVan_Gool2008" class="citation journal cs1">Bay, Herbert; Ess, Andreas; Tuytelaars, Tinne; Van Gool, Luc (2008). "Speeded-Up Robust Features (SURF)". <i>Computer Vision and Image Understanding</i>. <b>110</b> (3): 346–359. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.205.738">10.1.1.205.738</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.cviu.2007.09.014">10.1016/j.cviu.2007.09.014</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:14777911">14777911</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computer+Vision+and+Image+Understanding&amp;rft.atitle=Speeded-Up+Robust+Features+%28SURF%29&amp;rft.volume=110&amp;rft.issue=3&amp;rft.pages=346-359&amp;rft.date=2008&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.205.738%23id-name%3DCiteSeerX&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A14777911%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cviu.2007.09.014&amp;rft.aulast=Bay&amp;rft.aufirst=Herbert&amp;rft.au=Ess%2C+Andreas&amp;rft.au=Tuytelaars%2C+Tinne&amp;rft.au=Van+Gool%2C+Luc&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://www.gizmag.com/learning-object-recognition-algorithm-byu/30512">"New object recognition algorithm learns on the fly"</a>. Gizmag.com. 20 January 2014<span class="reference-accessdate">. Retrieved <span class="nowrap">2014-01-21</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=New+object+recognition+algorithm+learns+on+the+fly&amp;rft.pub=Gizmag.com&amp;rft.date=2014-01-20&amp;rft_id=http%3A%2F%2Fwww.gizmag.com%2Flearning-object-recognition-algorithm-byu%2F30512&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFLillywhiteLeeTippettsArchibald2013" class="citation journal cs1">Lillywhite, K.; Lee, D. J.; Tippetts, B.; Archibald, J. (2013). "A feature construction method for general object recognition". <i>Pattern Recognition</i>. <b>46</b> (12): 3300. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2013PatRe..46.3300L">2013PatRe..46.3300L</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.patcog.2013.06.002">10.1016/j.patcog.2013.06.002</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition&amp;rft.atitle=A+feature+construction+method+for+general+object+recognition&amp;rft.volume=46&amp;rft.issue=12&amp;rft.pages=3300&amp;rft.date=2013&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patcog.2013.06.002&amp;rft_id=info%3Abibcode%2F2013PatRe..46.3300L&amp;rft.aulast=Lillywhite&amp;rft.aufirst=K.&amp;rft.au=Lee%2C+D.+J.&amp;rft.au=Tippetts%2C+B.&amp;rft.au=Archibald%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">Brown, Matthew, and David G. Lowe. "<a rel="nofollow" class="external text" href="http://www.cs.ubc.ca/~lowe/papers/brown05.pdf">Unsupervised 3D object recognition and reconstruction in unordered datasets</a>." 3-D Digital Imaging and Modeling, 2005. 3DIM 2005. Fifth International Conference on. IEEE, 2005.</span>
</li>
<li id="cite_note-Aude-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-Aude_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Aude_12-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Oliva, Aude, and Antonio Torralba. "<a rel="nofollow" class="external text" href="http://people.csail.mit.edu/torralba/publications/reviewPapers/TrendsInCog.pdf">The role of context in object recognition</a>." Trends in cognitive sciences 11.12 (2007): 520-527.</span>
</li>
<li id="cite_note-Niu-13"><span class="mw-cite-backlink">^ <a href="#cite_ref-Niu_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Niu_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Niu, Zhenxing, et al. "<a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.667.7037&amp;rep=rep1&amp;type=pdf">Context aware topic model for scene recognition</a>." 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012.</span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text">Stein, Fridtjof, and Gérard Medioni. "<a rel="nofollow" class="external text" href="http://graphics.stanford.edu/~smr/ICP/comparison/stein-medioni-reg-pami92.pdf">Structural indexing: Efficient 3-D object recognition</a>." IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 2 (1992): 125-145.</span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text">Zhu, Song-Chun, and David Mumford. "<a rel="nofollow" class="external text" href="http://www.nowpublishers.com/article/DownloadSummary/CGV-018">A stochastic grammar of images</a>." Foundations and Trends in Computer Graphics and Vision 2.4 (2007): 259-362.</span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">Nayar, Shree K., and Ruud M. Bolle. "<a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.6253&amp;rep=rep1&amp;type=pdf">Reflectance based object recognition</a>." International journal of computer vision 17.3 (1996): 219-240.</span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text">Worthington, Philip L., and Edwin R. Hancock. "<a rel="nofollow" class="external text" href="http://eprints.whiterose.ac.uk/archive/00001989/01/hancocker5.pdf">Object recognition using shape-from-shading</a>." IEEE Transactions on Pattern Analysis and Machine Intelligence 23.5 (2001): 535-542.</span>
</li>
<li id="cite_note-Shotton-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-Shotton_18-0">^</a></b></span> <span class="reference-text">Shotton, Jamie, et al. "<a rel="nofollow" class="external text" href="https://www.researchgate.net/profile/Carsten_Rother/publication/220660505_TextonBoost_for_Image_Understanding_Multi-Class_Object_Recognition_and_Segmentation_by_Jointly_Modeling_Texture_Layout_and_Context/links/54f6e9c30cf21d8b8a5ec6e0.pdf">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</a>." International journal of computer vision 81.1 (2009): 2-23.</span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://www.kurzweilai.net/better-robot-vision">"Better robot vision"</a>. KurzweilAI<span class="reference-accessdate">. Retrieved <span class="nowrap">2013-10-09</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Better+robot+vision&amp;rft.pub=KurzweilAI&amp;rft_id=http%3A%2F%2Fwww.kurzweilai.net%2Fbetter-robot-vision&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text">Donahue, Jeffrey, et al. "<a rel="nofollow" class="external text" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf">Long-term recurrent convolutional networks for visual recognition and description</a>." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text">Karpathy, Andrej, and Li Fei-Fei. "<a rel="nofollow" class="external text" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf">Deep visual-semantic alignments for generating image descriptions</a>." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFP_DuyguluK_BarnardN_de_FretiasD_Forsyth2002" class="citation conference cs1">P Duygulu; K Barnard; N de Fretias &amp; D Forsyth (2002). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20050305174408/http://vision.cs.arizona.edu/kobus/research/publications/ECCV-02-1/">"Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary"</a>. <i>Proceedings of the European Conference on Computer Vision</i>. pp. 97–112. Archived from <a rel="nofollow" class="external text" href="http://vision.cs.arizona.edu/kobus/research/publications/ECCV-02-1/">the original</a> on 2005-03-05.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Object+recognition+as+machine+translation%3A+Learning+a+lexicon+for+a+fixed+image+vocabulary&amp;rft.btitle=Proceedings+of+the+European+Conference+on+Computer+Vision&amp;rft.pages=97-112&amp;rft.date=2002&amp;rft.au=P+Duygulu&amp;rft.au=K+Barnard&amp;rft.au=N+de+Fretias&amp;rft.au=D+Forsyth&amp;rft_id=http%3A%2F%2Fvision.cs.arizona.edu%2Fkobus%2Fresearch%2Fpublications%2FECCV-02-1%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://play.google.com/store/apps/details?id=com.davecote.androideyes/">"Android Eyes Computer Vision"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Android+Eyes+Computer+Vision&amp;rft_id=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.davecote.androideyes%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span>Martha J. Farah  "Visual Agnosia", Computer Vision Computing Cognitive Neuroscience, MIT Press, 2011-05-01, Pages 760-781, ISSN 1468-4233 <a rel="nofollow" class="external autonumber" href="https://play.google.com/store/apps/details?id=com.davecote.androideyes">[1]</a><sup class="noprint Inline-Template"><span style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Link_rot" title="Wikipedia:Link rot"><span title=" Dead link tagged November 2018">dead link</span></a></i>]</span></sup></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text">Esteva, Andre, et al. "<a rel="nofollow" class="external text" href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7822-andre-esteva-dermatologiest-level-classification-of-skin-cancer.pdf">Dermatologist-level classification of skin cancer with deep neural networks</a>." Nature 542.7639 (2017): 115.</span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">Brown, M., and Lowe, D.G., "<a rel="nofollow" class="external text" href="http://faculty.cse.tamu.edu/jchai/CPSC641/iccv2003.pdf">Recognising Panoramas</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20141225184429/http://faculty.cse.tamu.edu/jchai/CPSC641/iccv2003.pdf">Archived</a> 2014-12-25 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>," ICCV, p. 1218, Ninth IEEE International Conference on Computer Vision (ICCV'03) - Volume 2,  Nice,France, 2003</span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text">Li, L., Guo, B., and Shao, K., "<a rel="nofollow" class="external text" href="http://www.opticsjournal.net/ViewFull0.htm?aid=OJ070630000026rYu1x4">Geometrically robust image watermarking using scale-invariant feature transform and Zernike moments</a>," Chinese Optics Letters, Volume 5, Issue 6, pp. 332-335, 2007.</span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text">Se,S., Lowe, D.G., and Little, J.J.,"<a rel="nofollow" class="external text" href="http://142.103.6.5/nest/lci/papers/docs2005/se05.pdf">Vision-based global localization and mapping for mobile robots</a>", IEEE Transactions on Robotics, 21, 3 (2005), pp. 364-375.</span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text">Thomas Serre, Maximillian Riesenhuber, Jennifer Louie, Tomaso Poggio, "<a rel="nofollow" class="external text" href="https://apps.dtic.mil/sti/pdfs/ADA454940.pdf">On the Role of Object-Specific features for Real World Object Recognition in Biological Vision</a>." Artificial Intelligence Lab, and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Center for Biological and Computational Learning, Mc Govern Institute for Brain Research, Cambridge, MA, USA</span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFPermaloffGrafton1992" class="citation journal cs1">Permaloff, Anne; Grafton, Carl (1992). <a rel="nofollow" class="external text" href="https://www.jstor.org/stable/419444">"Optical Character Recognition"</a>. <i>PS: Political Science and Politics</i>. <b>25</b> (3): 523–531. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.2307%2F419444">10.2307/419444</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a> <a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1049-0965">1049-0965</a>. <a href="/wiki/JSTOR_(identifier)" class="mw-redirect" title="JSTOR (identifier)">JSTOR</a> <a rel="nofollow" class="external text" href="https://www.jstor.org/stable/419444">419444</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:64806776">64806776</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PS%3A+Political+Science+and+Politics&amp;rft.atitle=Optical+Character+Recognition&amp;rft.volume=25&amp;rft.issue=3&amp;rft.pages=523-531&amp;rft.date=1992&amp;rft.issn=1049-0965&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A64806776%23id-name%3DS2CID&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F419444%23id-name%3DJSTOR&amp;rft_id=info%3Adoi%2F10.2307%2F419444&amp;rft.aulast=Permaloff&amp;rft.aufirst=Anne&amp;rft.au=Grafton%2C+Carl&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F419444&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text">Christian Demant, Bernd Streicher-Abel, Peter Waszkewitz, "Industrial image processing: visual quality control in manufacturing" <i><a rel="nofollow" class="external text" href="https://books.google.com/books?id=cfwBx7NuRdwC&amp;pg=5">Outline of object recognition</a></i> at <a href="/wiki/Google_Books" title="Google Books">Google Books</a></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text">Nuno Vasconcelos "<a rel="nofollow" class="external text" href="http://www.svcl.ucsd.edu/publications/conference/2001/cvpr01/cvpr01.pdf">Image Indexing with Mixture Hierarchies</a>" <a rel="nofollow" class="external text" href="https://web.archive.org/web/20110118231444/http://www.svcl.ucsd.edu/publications/conference/2001/cvpr01/cvpr01.pdf">Archived</a> 2011-01-18 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a> Compaq Computer Corporation, Proc. IEEE Conference in Computer Vision and Pattern Recognition, Kauai, Hawaii, 2001</span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFHeikkiläSilvén2004" class="citation journal cs1">Heikkilä, Janne; Silvén, Olli (2004). "A real-time system for monitoring of cyclists and pedestrians". <i>Image and Vision Computing</i>. <b>22</b> (7): 563–570. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.imavis.2003.09.010">10.1016/j.imavis.2003.09.010</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Image+and+Vision+Computing&amp;rft.atitle=A+real-time+system+for+monitoring+of+cyclists+and+pedestrians&amp;rft.volume=22&amp;rft.issue=7&amp;rft.pages=563-570&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1016%2Fj.imavis.2003.09.010&amp;rft.aulast=Heikkil%C3%A4&amp;rft.aufirst=Janne&amp;rft.au=Silv%C3%A9n%2C+Olli&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFJungKimYoonKim2006" class="citation book cs1">Jung, Ho Gi; Kim, Dong Suk; Yoon, Pal Joo; Kim, Jaihie (2006). "Structure Analysis Based Parking Slot Marking Recognition for Semi-automatic Parking System". In Yeung, Dit-Yan; Kwok, James T.; Fred, Ana; Roli, Fabio; de Ridder, Dick (eds.). <i>Structural, Syntactic, and Statistical Pattern Recognition</i>. Lecture Notes in Computer Science. Vol. 4109. Berlin, Heidelberg: Springer. pp. 384–393. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F11815921_42">10.1007/11815921_42</a></span>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-3-540-37241-7" title="Special:BookSources/978-3-540-37241-7"><bdi>978-3-540-37241-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Structure+Analysis+Based+Parking+Slot+Marking+Recognition+for+Semi-automatic+Parking+System&amp;rft.btitle=Structural%2C+Syntactic%2C+and+Statistical+Pattern+Recognition&amp;rft.place=Berlin%2C+Heidelberg&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=384-393&amp;rft.pub=Springer&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1007%2F11815921_42&amp;rft.isbn=978-3-540-37241-7&amp;rft.aulast=Jung&amp;rft.aufirst=Ho+Gi&amp;rft.au=Kim%2C+Dong+Suk&amp;rft.au=Yoon%2C+Pal+Joo&amp;rft.au=Kim%2C+Jaihie&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text">S. K. Nayar, H. Murase, and S.A. Nene, "<a rel="nofollow" class="external text" href="http://www1.cs.columbia.edu/CAVE/publications/pdfs/Nayar_ICRA94.pdf">Learning, Positioning, and tracking Visual appearance</a>" Proc. Of IEEE Intl. Conf. on Robotics and Automation, San Diego, May 1994</span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFLiuGleicherJinAgarwala2009" class="citation journal cs1">Liu, F.; Gleicher, M.; Jin, H.; Agarwala, A. (2009). "Content-preserving warps for 3D video stabilization". <i>ACM Transactions on Graphics</i>. <b>28</b> (3): 1. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.678.3088">10.1.1.678.3088</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F1531326.1531350">10.1145/1531326.1531350</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ACM+Transactions+on+Graphics&amp;rft.atitle=Content-preserving+warps+for+3D+video+stabilization&amp;rft.volume=28&amp;rft.issue=3&amp;rft.pages=1&amp;rft.date=2009&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.678.3088%23id-name%3DCiteSeerX&amp;rft_id=info%3Adoi%2F10.1145%2F1531326.1531350&amp;rft.aulast=Liu&amp;rft.aufirst=F.&amp;rft.au=Gleicher%2C+M.&amp;rft.au=Jin%2C+H.&amp;rft.au=Agarwala%2C+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOutline+of+object+recognition" class="Z3988"></span></span>
</li>
</ol></div>
<div class="mw-heading mw-heading2"><h2 id="References">References</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=26" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li>Elgammal, Ahmed  <a rel="nofollow" class="external text" href="http://www.cs.rutgers.edu/~elgammal/classes/cs534/lectures/3D_modelbasedvision.pdf">"CS 534: Computer Vision 3D Model-based recognition"</a>,  Dept of Computer Science, Rutgers University;</li>
<li>Hartley, Richard and Zisserman, Andrew  <a rel="nofollow" class="external text" href="http://www8.cs.umu.se/kurser/TDBD09/VT02/cvbook/ch20csppose.pdf">"Multiple View Geometry in computer vision"</a>, Cambridge Press, 2000, <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/0-521-62304-9" title="Special:BookSources/0-521-62304-9">0-521-62304-9</a>.</li>
<li>Roth, Peter M. and Winter, Martin  <a rel="nofollow" class="external text" href="http://web.mit.edu/~wingated/www/introductions/appearance_based_methods.pdf">"Survey of Appearance-Based Methods for Object Recognition",  Technical Report ICG-TR-01/08</a>, Inst. for Computer Graphics and Vision, Graz University of Technology, Austria;  January 15, 2008.</li>
<li>Collins, Robert  <a rel="nofollow" class="external text" href="http://www.cse.psu.edu/~rcollins/CSE486/lecture31_6pp.pdf">"Lecture 31: Object Recognition: SIFT Keys"</a>,  CSE486, Penn State</li>
<li><a rel="nofollow" class="external text" href="http://www.iprg.co.in">IPRG</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20201228051352/http://www.iprg.co.in/">Archived</a> 2020-12-28 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a> Image Processing - Online Open Research Group</li>
<li><a rel="nofollow" class="external text" href="http://research.google.com/pubs/ChristianSzegedy.html">Christian Szegedy</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20150906143037/http://research.google.com/pubs/ChristianSzegedy.html">Archived</a> 2015-09-06 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>, <a rel="nofollow" class="external text" href="http://research.google.com/pubs/AlexanderToshev.html">Alexander Toshev</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20151004143144/http://research.google.com/pubs/AlexanderToshev.html">Archived</a> 2015-10-04 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a> and <a rel="nofollow" class="external text" href="http://www.dumitru.ca/">Dumitru Erhan</a>. <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf">Deep Neural Networks for Object Detection</a>. <a rel="nofollow" class="external text" href="http://papers.nips.cc/book/advances-in-neural-information-processing-systems-26-2013">Advances in Neural Information Processing Systems 26</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20200905031104/https://papers.nips.cc/book/advances-in-neural-information-processing-systems-26-2013">Archived</a> 2020-09-05 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>, 2013. page 2553–2561.</li></ul>
<div class="mw-heading mw-heading2"><h2 id="External_links">External links</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Outline_of_object_recognition&amp;action=edit&amp;section=27" title="Edit section: External links"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1235681985">.mw-parser-output .side-box{margin:4px 0;box-sizing:border-box;border:1px solid #aaa;font-size:88%;line-height:1.25em;background-color:var(--background-color-interactive-subtle,#f8f9fa);display:flow-root}.mw-parser-output .side-box-abovebelow,.mw-parser-output .side-box-text{padding:0.25em 0.9em}.mw-parser-output .side-box-image{padding:2px 0 2px 0.9em;text-align:center}.mw-parser-output .side-box-imageright{padding:2px 0.9em 2px 0;text-align:center}@media(min-width:500px){.mw-parser-output .side-box-flex{display:flex;align-items:center}.mw-parser-output .side-box-text{flex:1;min-width:0}}@media(min-width:720px){.mw-parser-output .side-box{width:238px}.mw-parser-output .side-box-right{clear:right;float:right;margin-left:1em}.mw-parser-output .side-box-left{margin-right:1em}}</style><style data-mw-deduplicate="TemplateStyles:r1236088121">.mw-parser-output .sister-box .side-box-abovebelow{padding:0.75em 0;text-align:center}.mw-parser-output .sister-box .side-box-abovebelow>b{display:block}.mw-parser-output .sister-box .side-box-text>ul{border-top:1px solid #aaa;padding:0.75em 0;width:217px;margin:0 auto}.mw-parser-output .sister-box .side-box-text>ul>li{min-height:31px}.mw-parser-output .sister-logo{display:inline-block;width:31px;line-height:31px;vertical-align:middle;text-align:center}.mw-parser-output .sister-link{display:inline-block;margin-left:4px;width:182px;vertical-align:middle}@media print{body.ns-0 .mw-parser-output .sistersitebox{display:none!important}}</style><div role="navigation" aria-labelledby="sister-projects" class="side-box metadata side-box-right sister-box sistersitebox plainlinks"><style data-mw-deduplicate="TemplateStyles:r1126788409">.mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0;padding:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}</style>
<div class="side-box-abovebelow">
<b>Object recognition</b>  at Wikipedia's <a href="/wiki/Wikipedia:Wikimedia_sister_projects" title="Wikipedia:Wikimedia sister projects"><span id="sister-projects">sister projects</span></a></div>
<div class="side-box-flex">
<div class="side-box-text plainlist"><ul><li><span class="sister-logo"><span class="mw-valign-middle" typeof="mw:File"><span><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/27px-Wiktionary-logo-v2.svg.png" decoding="async" width="27" height="27" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/41px-Wiktionary-logo-v2.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/54px-Wiktionary-logo-v2.svg.png 2x" data-file-width="391" data-file-height="391"></span></span></span><span class="sister-link"><a href="https://en.wiktionary.org/wiki/Special:Search/Object_recognition" class="extiw" title="wikt:Special:Search/Object recognition">Definitions</a> from Wiktionary</span></li><li><span class="sister-logo"><span class="mw-valign-middle" typeof="mw:File"><span><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png" decoding="async" width="20" height="27" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/40px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376"></span></span></span><span class="sister-link"><a href="https://commons.wikimedia.org/wiki/Special:Search/Object_recognition" class="extiw" title="c:Special:Search/Object recognition">Media</a> from Commons</span></li><li><span class="sister-logo"><span class="mw-valign-middle" typeof="mw:File"><span><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/27px-Wikinews-logo.svg.png" decoding="async" width="27" height="15" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/41px-Wikinews-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/54px-Wikinews-logo.svg.png 2x" data-file-width="759" data-file-height="415"></span></span></span><span class="sister-link"><a href="https://en.wikinews.org/wiki/Special:Search/Object_recognition" class="extiw" title="n:Special:Search/Object recognition">News</a> from Wikinews</span></li><li><span class="sister-logo"><span class="mw-valign-middle" typeof="mw:File"><span><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/23px-Wikiquote-logo.svg.png" decoding="async" width="23" height="27" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/35px-Wikiquote-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/46px-Wikiquote-logo.svg.png 2x" data-file-width="300" data-file-height="355"></span></span></span><span class="sister-link"><a href="https://en.wikiquote.org/wiki/Special:Search/Object_recognition" class="extiw" title="q:Special:Search/Object recognition">Quotations</a> from Wikiquote</span></li><li><span class="sister-logo"><span class="mw-valign-middle" typeof="mw:File"><span><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/26px-Wikisource-logo.svg.png" decoding="async" width="26" height="27" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/39px-Wikisource-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/51px-Wikisource-logo.svg.png 2x" data-file-width="410" data-file-height="430"></span></span></span><span class="sister-link"><a href="https://en.wikisource.org/wiki/Special:Search/Object_recognition" class="extiw" title="s:Special:Search/Object recognition">Texts</a> from Wikisource</span></li><li><span class="sister-logo"><span class="mw-valign-middle" typeof="mw:File"><span><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/27px-Wikibooks-logo.svg.png" decoding="async" width="27" height="27" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/41px-Wikibooks-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/54px-Wikibooks-logo.svg.png 2x" data-file-width="300" data-file-height="300"></span></span></span><span class="sister-link"><a href="https://en.wikibooks.org/wiki/Special:Search/Object_recognition" class="extiw" title="b:Special:Search/Object recognition">Textbooks</a> from Wikibooks</span></li><li><span class="sister-logo"><span class="mw-valign-middle" typeof="mw:File"><span><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Wikiversity_logo_2017.svg/27px-Wikiversity_logo_2017.svg.png" decoding="async" width="27" height="22" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Wikiversity_logo_2017.svg/41px-Wikiversity_logo_2017.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Wikiversity_logo_2017.svg/54px-Wikiversity_logo_2017.svg.png 2x" data-file-width="626" data-file-height="512"></span></span></span><span class="sister-link"><a href="https://en.wikiversity.org/wiki/Special:Search/Object_recognition" class="extiw" title="v:Special:Search/Object recognition">Resources</a> from Wikiversity</span></li></ul></div></div>
</div>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1236075235">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}</style></div><div role="navigation" class="navbox" aria-labelledby="Wikipedia_outlines" style="padding:3px"><table class="nowraplinks mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1239400231"><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Outline_footer" title="Template:Outline footer"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Outline_footer" title="Template talk:Outline footer"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Outline_footer" title="Special:EditPage/Template:Outline footer"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Wikipedia_outlines" style="font-size:114%;margin:0 4em"><a href="/wiki/Wikipedia:Contents/Outlines" title="Wikipedia:Contents/Outlines">Wikipedia outlines</a></div></th></tr><tr><td colspan="2" class="navbox-list navbox-odd hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Wikipedia:Contents/Outlines#General_reference" title="Wikipedia:Contents/Outlines">General reference</a></li></ul>
<ul><li><a href="/wiki/Wikipedia:Contents/Outlines#Culture_and_the_arts" title="Wikipedia:Contents/Outlines">Culture and the arts</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#Geography_and_places" title="Wikipedia:Contents/Outlines">Geography and places</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#Health_and_fitness" title="Wikipedia:Contents/Outlines">Health and fitness</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#History_and_events" title="Wikipedia:Contents/Outlines">History and events</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#Mathematics_and_logic" title="Wikipedia:Contents/Outlines">Mathematics and logic</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#Natural_and_physical_sciences" title="Wikipedia:Contents/Outlines">Natural and physical sciences</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#People_and_self" title="Wikipedia:Contents/Outlines">People and self</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#Philosophy_and_thinking" title="Wikipedia:Contents/Outlines">Philosophy and thinking</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#Religion_and_belief_systems" title="Wikipedia:Contents/Outlines">Religion and belief systems</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#Society_and_social_sciences" title="Wikipedia:Contents/Outlines">Society and social sciences</a></li>
<li><a href="/wiki/Wikipedia:Contents/Outlines#Technology_and_applied_sciences" title="Wikipedia:Contents/Outlines">Technology and applied sciences</a></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw‐web.codfw.main‐64999cd48f‐bxlzc
Cached time: 20240927010041
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.632 seconds
Real time usage: 0.776 seconds
Preprocessor visited node count: 2326/1000000
Post‐expand include size: 63553/2097152 bytes
Template argument size: 1934/2097152 bytes
Highest expansion depth: 17/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 95027/5000000 bytes
Lua time usage: 0.397/10.000 seconds
Lua memory usage: 6601517/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  677.203      1 -total
 22.82%  154.513     11 Template:Cite_journal
 21.77%  147.430      1 Template:Reflist
 16.02%  108.473      1 Template:FeatureDetectionCompVisNavbox
 15.49%  104.909      1 Template:Sidebar
 12.28%   83.134      1 Template:Short_description
  9.73%   65.868      1 Template:Sisterlinks
  6.85%   46.417      1 Template:Citation_needed
  6.80%   46.081      1 Template:Outline_footer
  6.68%   45.251      2 Template:Fix
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:14661466-0!canonical and timestamp 20240927010041 and revision id 1230387592. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> --><noscript><img src="https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Outline_of_object_recognition&amp;oldid=1230387592">https://en.wikipedia.org/w/index.php?title=Outline_of_object_recognition&amp;oldid=1230387592</a>"</div></div>