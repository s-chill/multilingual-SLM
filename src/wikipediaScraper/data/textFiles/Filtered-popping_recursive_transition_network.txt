A filtered-popping recursive transition network ( FPRTN ), or simply filtered-popping network ( FPN ), is a recursive transition network ( RTN ) extended with a map of states to keys where returning from a subroutine jump requires the acceptor and return states to be mapped to the same key. RTNs are finite-state machines that can be seen as finite-state automata extended with a stack of return states; as well as consuming transitions and ε {\displaystyle \varepsilon } -transitions, RTNs may define call transitions. These transitions perform a subroutine jump by pushing the transition's target state onto the stack and bringing the machine to the called state. Each time an acceptor state is reached, the return state at the top of the stack is popped out, provided that the stack is not empty, and the machine is brought to this state.

Throughout this article we refer to filtered-popping recursive transition networks as FPNs , though this acronym is ambiguous (e.g.: fuzzy Petri nets ). Filtered-popping networks and FPRTNs are unambiguous alternatives.

A FPN is a structure ( Q , K , Σ , δ , κ , Q I , F ) {\displaystyle (Q,K,\Sigma ,\delta ,\kappa ,Q_{I},F)} where

Transitions represent the possibility of bringing the FPN from a source state q s {\displaystyle q_{s}} to a target state q t {\displaystyle q_{t}} by possibly performing an additional action. Depending on this action, we distinguish the following types of explicitly -defined transitions:

The behaviour of call transitions is governed by two kinds of implicitly -defined transitions:

Push transitions initialize subroutine jumps and pop transitions are equivalent to return statements .

A ( natural language ) text can be enriched with meta-information by the application of a RTN with output ; for instance, a RTN inserting XML tags can be used for transforming a plain text into a structured XML document. A RTN with output representing a natural language grammar would delimit and add the syntactic structure of each text sentence (see parsing ). Other RTNs with output could simply mark text segments containing relevant information (see information extraction ). The application of a RTN with output representing an ambiguous grammar results in a set of possible translations or interpretations of the input. Computing this set has an exponential worst-case cost , even for an Earley parser for RTNs with output, due to cases in which the number of translations increases exponentially w.r.t. the input length; for instance, the number of interpretations of a natural language sentence increases exponentially w.r.t. the number of unresolved prepositional phrase attachments:

FPNs serve as a compact representation of this set of translations, allowing to compute it in cubic time by means of an Earley-like parser. FPN states correspond to execution states (see instruction steps ) of an Earley-parser for RTNs without output, and FPN transitions correspond to possible translations of input symbols. The κ {\displaystyle \kappa } map of the resulting FPN gives the correspondence between the represented output segments and the recognized input segments: given a recognized input sequence σ 1 … σ l {\displaystyle \sigma _{1}\ldots \sigma _{l}} and a FPN path p {\displaystyle p} starting at a state q {\displaystyle q} and ending at a state q ′ {\displaystyle q^{\prime }} , p {\displaystyle p} represents a possible translation of input segment σ κ ( q ) + 1 … σ κ ( q ′ ) {\displaystyle \sigma _{\kappa (q)+1}\ldots \sigma _{\kappa (q^{\prime })}} . The filtered-popping feature is required in order to avoid FPN paths to represent translations of disconnected or overlapping input segments: a FPN call may contain several translation paths from the called state to an acceptor state, where the input segments they correspond to share the same start point but do not necessarily have the same length. Only return states corresponding to the same input point than the acceptor state finishing the call are valid return states.