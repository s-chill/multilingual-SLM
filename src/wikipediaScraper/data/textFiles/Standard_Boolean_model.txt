The (standard) Boolean model of information retrieval ( BIR ) is a classical information retrieval (IR) model and, at the same time, the first and most-adopted one. The BIR is based on Boolean logic and classical set theory in that both the documents to be searched and the user's query are conceived as sets of terms (a bag-of-words model ). Retrieval is based on whether or not the documents contain the query terms and whether they satisfy the boolean conditions described by the query.

An index term is a word or expression , which may be stemmed , describing or characterizing a document,  such as a keyword given for a journal article. Let T = { t 1 , t 2 , … , t n } {\displaystyle T=\{t_{1},t_{2},\ \ldots ,\ t_{n}\}} be the set of all such index terms.

A document is any subset of T {\displaystyle T} . Let D = { D 1 , … , D n } {\displaystyle D=\{D_{1},\ \ldots \ ,D_{n}\}} be the set of all documents.

T {\displaystyle T} is a series of words or small phrases (index terms). Each of those words or small phrases are named t n {\displaystyle t_{n}} , where n {\displaystyle n} is the number of the term in the series/list. You can think of T {\displaystyle T} as "Terms" and t n {\displaystyle t_{n}} as "index term n ".

The words or small phrases (index terms t n {\displaystyle t_{n}} ) can exist in documents. These documents then form a series/list D {\displaystyle D} where each individual documents are called D n {\displaystyle D_{n}} . These documents ( D n {\displaystyle D_{n}} ) can contain words or small phrases (index terms t n {\displaystyle t_{n}} ) such as D 1 {\displaystyle D_{1}} could contain the terms t 1 {\displaystyle t_{1}} and t 2 {\displaystyle t_{2}} from T {\displaystyle T} . There is an example of this in the following section.

Index terms generally want to represent words which have more meaning to them and corresponds to what the content of an article or document could talk about. Terms like "the" and "like" would appear in nearly all documents whereas "Bayesian" would only be a small fraction of documents. Therefor, rarer terms like "Bayesian" are a better choice to be selected in the T {\displaystyle T} sets. This relates to Entropy (information theory) . There are multiple types of operations that can be applied to index terms used in queries to make them more generic and more relevant. One such is Stemming .

A query is a Boolean expression Q {\textstyle Q} in normal form: Q = ( W 1 ∨ W 2 ∨ ⋯ ) ∧ ⋯ ∧ ( W i ∨ W i + 1 ∨ ⋯ ) {\displaystyle Q=(W_{1}\ \lor \ W_{2}\ \lor \ \cdots )\land \ \cdots \ \land \ (W_{i}\ \lor \ W_{i+1}\ \lor \ \cdots )} where W i {\textstyle W_{i}} is true for D j {\displaystyle D_{j}} when t i ∈ D j {\displaystyle t_{i}\in D_{j}} . (Equivalently, Q {\textstyle Q} could be expressed in disjunctive normal form .)

Any Q {\displaystyle Q} queries are a selection of index terms ( t n {\displaystyle t_{n}} or W n {\displaystyle W_{n}} ) picked from a set T {\displaystyle T} of terms which are combined using Boolean operators to form a set of conditions.

These conditions are then applied to a set D {\displaystyle D} of documents which contain the same index terms ( t n {\displaystyle t_{n}} ) from the set T {\displaystyle T} .

We seek to find the set of documents that satisfy Q {\textstyle Q} .  This operation is called retrieval and consists of the following two steps:

Let the set of original (real) documents be, for example

where

D 1 {\textstyle D_{1}} = "Bayes' principle: The principle that, in estimating a parameter, one should initially assume that each possible value has equal probability (a uniform prior  distribution)."

D 2 {\textstyle D_{2}} = " Bayesian decision theory : A mathematical theory of decision-making which presumes utility and probability functions, and according to which the act to be chosen is the Bayes act, i.e. the one with highest subjective expected utility. If one had unlimited time and calculating power with which to make every decision, this procedure would be the best way to make any decision."

D 3 {\textstyle D_{3}} = "Bayesian epistemology : A philosophical theory which holds that the epistemic status of a proposition (i.e. how well proven or well established it is) is best measured by a probability and that the proper way to revise this probability is given by Bayesian conditionalisation or similar procedures. A Bayesian epistemologist would use probability to define, and explore the relationship between, concepts such as epistemic status, support or explanatory power."

Let the set T {\textstyle T} of terms be:

T = { t 1 = Bayes' principle , t 2 = probability , t 3 = decision-making , t 4 = Bayesian epistemology } {\displaystyle T=\{t_{1}={\text{Bayes' principle}},t_{2}={\text{probability}},t_{3}={\text{decision-making}},t_{4}={\text{Bayesian epistemology}}\}}

Then, the set D {\textstyle D} of documents is as follows:

where D 1 = { probability , Bayes' principle } D 2 = { probability , decision-making } D 3 = { probability , Bayesian epistemology } {\displaystyle {\begin{aligned}D_{1}&=\{{\text{probability}},\ {\text{Bayes' principle}}\}\\D_{2}&=\{{\text{probability}},\ {\text{decision-making}}\}\\D_{3}&=\{{\text{probability}},\ {\text{Bayesian epistemology}}\}\end{aligned}}}

Let the query Q {\textstyle Q} be ("probability" AND "decision-making"):

Q = probability ∧ decision-making {\displaystyle Q={\text{probability}}\land {\text{decision-making}}} Then to retrieve the relevant documents:

This means that the original document D 2 {\displaystyle D_{2}} is the answer to Q {\textstyle Q} .

If there is more than one document with the same representation (the same subset of index terms t n {\displaystyle t_{n}} ), every such document is retrieved. Such documents are indistinguishable in the BIR (in other words, equivalent).

From a pure formal mathematical point of view, the BIR is straightforward. From a practical point of view, however, several further problems should be solved that relate to algorithms and data structures, such as, for example, the choice of terms (manual or automatic selection or both), stemming , hash tables , inverted file structure, and so on.

Another possibility is to use hash sets . Each document is represented by a hash table which contains every single term of that document. Since hash table size increases and decreases in real time with the addition and removal of terms, each document will occupy much less space in memory. However, it will have a slowdown in performance because the operations are more complex than with bit vectors . On the worst-case performance can degrade from O( n ) to O( n ). On the average case, the performance slowdown will not be that much worse than bit vectors and the space usage is much more efficient.

Each document can be summarized by Bloom filter representing the set of words in that document, stored in a fixed-length bitstring, called a signature.
The signature file contains one such superimposed code bitstring for every document in the collection.
Each query can also be summarized by a Bloom filter representing the set of words in the query, stored in a bitstring of the same fixed length.
The query bitstring is tested against each signature.

The signature file approached is used in BitFunnel .

An inverted index file contains two parts:
a vocabulary containing all the terms used in the collection,
and for each distinct term an inverted index that lists every document that mentions that term.