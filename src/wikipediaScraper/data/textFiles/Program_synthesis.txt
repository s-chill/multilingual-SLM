In computer science , program synthesis is the task to construct a program that provably satisfies a given high-level formal specification . In contrast to program verification , the program is to be constructed rather than given; however, both fields make use of formal proof techniques, and both comprise approaches of different degrees of automation. In contrast to automatic programming techniques, specifications in program synthesis are usually non- algorithmic statements in an appropriate logical calculus .

The primary application of program synthesis is to relieve the programmer of the burden of writing correct, efficient code that satisfies a specification. However, program synthesis also has applications to superoptimization and inference of loop invariants .

During the Summer Institute of Symbolic Logic at Cornell University in 1957, Alonzo Church defined the problem to synthesize a circuit from mathematical requirements. Even though the work only refers to circuits and not programs, the work is considered to be one of the earliest descriptions of program synthesis and some researchers refer to program synthesis as "Church's Problem". In the 1960s, a similar idea for an "automatic programmer" was explored by researchers in artificial intelligence.

Since then, various research communities considered the problem of program synthesis. Notable works include the 1969 automata-theoretic approach by Büchi and Landweber , and the works by Manna and Waldinger (c. 1980). The development of modern high-level programming languages can also be understood as a form of program synthesis.

The early 21st century has seen a surge of practical interest in the idea of program synthesis in the formal verification community and related fields. Armando Solar-Lezama showed that it is possible to encode program synthesis problems in Boolean logic and use algorithms for the Boolean satisfiability problem to automatically find programs.

In 2013, a unified framework for program synthesis problems called Syntax-guided Synthesis (stylized SyGuS) was proposed by researchers at UPenn , UC Berkeley , and MIT . The input to a SyGuS algorithm consists of a logical specification along with a context-free grammar of expressions that constrains the syntax of valid solutions. For example, to synthesize a function f that returns the maximum of two integers, the logical specification might look like this:

( f ( x , y ) = x ∨ f ( x , y ) = y ) ∧ f ( x , y ) ≥ x ∧ f ( x , y ) ≥ y

and the grammar might be:

where "ite" stands for "if-then-else". The expression

would be a valid solution, because it conforms to the grammar and the specification.

From 2014 through 2019, the yearly Syntax-Guided Synthesis Competition (or SyGuS-Comp) compared the different algorithms for program synthesis in a competitive event. The competition used a standardized input format, SyGuS-IF, based on SMT-Lib 2 . For example, the following SyGuS-IF encodes the problem of synthesizing the maximum of two integers (as presented above):

A compliant solver might return the following output:

Counter-example guided inductive synthesis (CEGIS) is an effective approach to building sound program synthesizers. CEGIS involves the interplay of two components: a generator which generates candidate programs, and a verifier which checks whether the candidates satisfy the specification.

Given a set of inputs I , a set of possible programs P , and a specification S , the goal of program synthesis is to find a program p in P such that for all inputs i in I , S ( p , i ) holds. CEGIS is parameterized over a generator and a verifier:

CEGIS runs the generator and verifier run in a loop, accumulating counter-examples:

Implementations of CEGIS typically use SMT solvers as verifiers.

CEGIS was inspired by counterexample-guided abstraction refinement (CEGAR).

The framework of Manna and Waldinger , published in 1980, starts from a user-given first-order specification formula . For that formula, a proof is constructed, thereby also synthesizing a functional program from unifying substitutions.

The framework is presented in a table layout, the columns containing:

Initially, background knowledge, pre-conditions, and post-conditions are entered into the table. After that, appropriate proof rules are applied manually. The framework has been designed to enhance human readability of intermediate formulas: contrary to classical resolution , it does not require clausal normal form , but allows one to reason with formulas of arbitrary structure and containing any junctors (" non-clausal resolution "). The proof is complete when t r u e {\displaystyle {\it {true}}} has been derived in the Goals column, or, equivalently, f a l s e {\displaystyle {\it {false}}} in the Assertions column. Programs obtained by this approach are guaranteed to satisfy the specification formula started from; in this sense they are correct by construction . Only a minimalist, yet Turing-complete , purely functional programming language , consisting of conditional, recursion, and arithmetic and other operators is supported.
Case studies performed within this framework synthesized algorithms to compute e.g. division , remainder , square root , term unification , answers to relational database queries and several sorting algorithms .

Proof rules include:

Murray has shown these rules to be complete for first-order logic . In 1986, Manna and Waldinger added generalized E-resolution and paramodulation rules to handle also equality; later, these rules turned out to be incomplete (but nevertheless sound ).

As a toy example, a functional program to compute the maximum M {\displaystyle M} of two numbers x {\displaystyle x} and y {\displaystyle y} can be derived as follows.

Starting from the requirement description " The maximum is larger than or equal to any given number, and is one of the given numbers ", the first-order formula ∀ X ∀ Y ∃ M : X ≤ M ∧ Y ≤ M ∧ ( X = M ∨ Y = M ) {\displaystyle \forall X\forall Y\exists M:X\leq M\land Y\leq M\land (X=M\lor Y=M)} is obtained as its formal translation. This formula is to be proved. By reverse Skolemization , the specification in line 10 is obtained, an upper- and lower-case letter denoting a variable and a Skolem constant , respectively.

After applying a transformation rule for the distributive law in line 11, the proof goal is a disjunction, and hence can be split into two cases, viz. lines 12 and 13.

Turning to the first case, resolving line 12 with the axiom in line 1 leads to instantiation of the program variable M {\displaystyle M} in line 14. Intuitively, the last conjunct of line 12 prescribes the value that M {\displaystyle M} must take in this case. Formally, the non-clausal resolution rule shown in line 57 above is applied to lines 12 and 1, with

yielding ¬ ( {\displaystyle \lnot (} true ∧ false ) ∧ ( x ≤ x ∧ y ≤ x ∧ true ) {\displaystyle )} ,
which simplifies to x ≤ x ∧ y ≤ x {\displaystyle x\leq x\land y\leq x} .

In a similar way, line 14 yields line 15 and then line 16 by resolution. Also, the second case, x ≤ M ∧ y ≤ M ∧ y = M {\displaystyle x\leq M\land y\leq M\land y=M} in line 13, is handled similarly, yielding eventually line 18.

In a last step, both cases (i.e. lines 16 and 18) are joined, using the resolution rule from line 58; to make that rule applicable, the preparatory step 15→16 was needed. Intuitively, line 18 could be read as "in case x ≤ y {\displaystyle x\leq y} , the output y {\displaystyle y} is valid (with respect to the original specification), while line 15 says "in case y ≤ x {\displaystyle y\leq x} , the output x {\displaystyle x} is valid; the step 15→16 established that both cases 16 and 18 are complementary. Since both line 16 and 18 comes with a program term, a conditional expression results in the program column. Since the goal formula true {\displaystyle {\textit {true}}} has been derived, the proof is done, and the program column of the " true {\displaystyle {\textit {true}}} " line contains the program.